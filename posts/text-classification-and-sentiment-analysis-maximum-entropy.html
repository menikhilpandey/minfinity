<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>minfinity | Text Classification and Sentiment Analysis : Maximum Entropy</title>
  <meta name="description" content="Discussion of theory behind popular Maximum Entropy Classifier in context of Natural Language Processing">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Text Classification and Sentiment Analysis : Maximum Entropy">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://blog.nikhilpandey.me/posts/text-classification-and-sentiment-analysis-maximum-entropy">
  <meta property="og:description" content="Discussion of theory behind popular Maximum Entropy Classifier in context of Natural Language Processing">
  <meta property="og:site_name" content="minfinity">
  <meta property="og:image" content="http://blog.nikhilpandey.me/assets/post-images/nlp-word-cloud.jpg">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://blog.nikhilpandey.me/posts/text-classification-and-sentiment-analysis-maximum-entropy">
  <meta name="twitter:title" content="Text Classification and Sentiment Analysis : Maximum Entropy">
  <meta name="twitter:description" content="Discussion of theory behind popular Maximum Entropy Classifier in context of Natural Language Processing">
  <meta name="twitter:image" content="http://blog.nikhilpandey.me/assets/post-images/nlp-word-cloud.jpg">

  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://blog.nikhilpandey.me/feed.xml" type="application/rss+xml" rel="alternate" title="minfinity Last 10 blog posts" />

  
    <link type="text/css" rel="stylesheet" href="/assets/light.css">
  
</head>

<body>
  <main role="main">
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav reveal">
  <a href="/" class="header-logo" title="minfinity">minfinity</a>
  <ul class="header-links">
    
      <li>
        <a href="https://twitter.com/menikhilpandey" target="_blank" title="Twitter">
          <span class="icon icon-social-twitter"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://www.facebook.com/menikhilpandey" target="_blank" title="Facebook">
          <span class="icon icon-social-facebook"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://github.com/menikhilpandey" target="_blank" title="GitHub">
          <span class="icon icon-social-github"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://www.linkedin.com/in/menikhilpandey" target="_blank" title="LinkedIn">
          <span class="icon icon-social-linkedin"></span>
        </a>
      </li>
    
    
    
      <li>
        <a href="mailto:contact@nikhilpandey.me" target="_blank" title="Email">
          <span class="icon icon-at"></span>
        </a>
      </li>
    
    
      <li>
        <a href="/feed.xml" target="_blank" title="RSS">
          <span class="icon icon-social-rss"></span>
        </a>
      </li>
    
  </ul>
</nav>

        <article class="article reveal">
          <header class="article-header">
            <h1>Text Classification and Sentiment Analysis : Maximum Entropy</h1>
            <p>Discussion of theory behind popular Maximum Entropy Classifier in context of Natural Language Processing</p>
            <div class="article-list-footer">
              <span class="article-list-date">
                February 19, 2017
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
                
                
                  3 minute read
                
              </span>
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                  <a href="/tag/machine-learning">machine-learning</a>
                
                  <a href="/tag/sentiment-analytics">sentiment-analytics</a>
                
              </div>
            </div>
          </header>

          <div class="article-content">
            <p>The principle behind Maximum Entropy is that the correct distribution is the one that maximizes the Entropy / uncertainty and still meets the constraints which are set by the ‘evidence’.</p>

<p>Let me explain this a bit more. In Information Theory, the word Entropy is used as a unit of measure for the unpredictability of the content of information. If you would throw a fair dice, each of the six outcomes have the same probability of occuring (1/6). Therefore you have maximum uncertainty; an entropy of 1. If the dice is weighted you already know one of the six outcomes has a higher probability of occuring and the uncertainty becomes less. If the dice is weighted so much that the outcome is always six, there is zero uncertainty in the outcome and hence the information entropy is also zero.
The same applies to letters in a word (or words in a sentence): if you assume that every letter has the same probability of occuring you have maximum uncertainty in predicting the next letter. But if you know that letters like E, A, O or I have a higher probability of occuring you have less uncertainty.</p>

<p>Knowing this, we can say that complex data has a high entropy, patterns and trends have lower entropy, information you know for a fact to be true has zero entropy (and therefore can be excluded).
The idea behind Maximum Entropy is that you want a model which is as unbiased as possible; events which are not excluded by known constraints should be assigned as much uncertainty as possible, meaning the probability distribution should be as uniform as possible. You are looking for the maximum value of the Entropy. If this is not entirely clear, I recommend you to read through this example.</p>

<p>The mathematical formula for Entropy is given by</p>

<p><a href="/assets/post-images/NLP-Max-Entropy/1.svg" class="fluidbox-trigger">
  <img src="/assets/post-images/NLP-Max-Entropy/1.svg" alt="Equation" />
</a></p>

<p>so the most likely probability distribution p is the one that maximizes this entropy:</p>

<p><a href="/assets/post-images/NLP-Max-Entropy/2.svg" class="fluidbox-trigger">
  <img src="/assets/post-images/NLP-Max-Entropy/2.svg" alt="Equation" />
</a></p>

<p>It can be shown that the probability distribution has an exponential form and hence is given by:</p>

<p><a href="/assets/post-images/NLP-Max-Entropy/3.svg" class="fluidbox-trigger">
  <img src="/assets/post-images/NLP-Max-Entropy/3.svg" alt="Equation" />
</a></p>

<p>where <strong><em>f<sub>i</sub> (d,c)</em></strong> is a feature function, <strong><em>lambda<sub>i</sub></em></strong>   is the weight parameter of the feature function and <strong><em>Z(d)</em></strong> is a normalization factor given by</p>

<p><a href="/assets/post-images/NLP-Max-Entropy/4.svg" class="fluidbox-trigger">
  <img src="/assets/post-images/NLP-Max-Entropy/4.svg" alt="Equation" />
</a></p>

<p>This feature function is an indicator function, which is expresses the expected value of the chosen statistics (words) in the training set. These feature functions can then be taken as constraints for the classification of the actual dataset (by eliminating the probability distributions <strong><em>P(c|d)</em></strong> which do not fit with these constraints).</p>

<p>Usually, the weight parameters are automatically determined by the Improved Iterative Scaling algorithm. This is simply a gradient descent function which can be iterated over until it converges to the global maximum. The pseudocode for the this algorithm is as follows:</p>

<ol>
  <li>Initialize all weight parameters lambda<sub>i</sub>   to zero.</li>
  <li>Repeat until convergence:
    <ul>
      <li>calculate the probability distribution <strong><em>P<sub>lambda</sub>(c|d)</em></strong>  with the weight parameters filled in.</li>
      <li>for each parameter lambda<sub>i</sub>   calculate change in lambda<sub>i</sub>. This is the solution to:
<a href="/assets/post-images/NLP-Max-Entropy/5.svg" class="fluidbox-trigger">
<img src="/assets/post-images/NLP-Max-Entropy/5.svg" alt="Equation" />
</a></li>
    </ul>
  </li>
</ol>

<ul>
  <li>update the value for the weight parameter:
<a href="/assets/post-images/NLP-Max-Entropy/6.svg" class="fluidbox-trigger">
<img src="/assets/post-images/NLP-Max-Entropy/6.svg" alt="Equation" />
</a></li>
</ul>

<p>In step 2b <strong><em>f(d,c)</em></strong> is given by the sum of all features in the training dataset <strong><em>d</em></strong>:</p>

<p><a href="/assets/post-images/NLP-Max-Entropy/7.svg" class="fluidbox-trigger">
  <img src="/assets/post-images/NLP-Max-Entropy/7.svg" alt="Equation" />
</a></p>

<p>Maximum Entropy is a general statistical classification algorithm and can be used to estimate any probability distribution. For the specific case of text classification, we can limit its form a bit more by using word counts as features:</p>

<p><a href="/assets/post-images/NLP-Max-Entropy/8.svg" class="fluidbox-trigger">
  <img src="/assets/post-images/NLP-Max-Entropy/8.svg" alt="Equation" />
</a></p>


          </div>

          <div class="article-share">
            
            <a href="" title="Share on Twitter" onclick="window.open('https://twitter.com/home?status=Text Classification and Sentiment Analysis : Ma... - http://blog.nikhilpandey.me/posts/text-classification-and-sentiment-analysis-maximum-entropy by @menikhilpandey', 'newwindow', 'width=500, height=225'); return false;">
              <span class="icon icon-social-twitter"></span>
            </a>
            <a href="" title="Share on Facebook" onclick="window.open('https://www.facebook.com/sharer/sharer.php?u=http://blog.nikhilpandey.me/posts/text-classification-and-sentiment-analysis-maximum-entropy', 'newwindow', 'width=500, height=500'); return false;">
              <span class="icon icon-social-facebook"></span>
            </a>
            <a href="" title="Share on Google+" onclick="window.open('https://plus.google.com/share?url=http://blog.nikhilpandey.me/posts/text-classification-and-sentiment-analysis-maximum-entropy', 'newwindow', 'width=550, height=400'); return false;">
              <span class="icon icon-social-googleplus"></span>
            </a>
          </div>

          
        </article>
        <footer class="footer reveal">
  <p>
    Powered by Jekyll.<br>
    © 2016 | <a href="http://nikhilpandey.me">Nikhil Pandey</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  <script type="text/javascript" src="/assets/vendor.js"></script>
<script type="text/javascript" src="/assets/application.js"></script>

<script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.16/webfont.js"></script>
<script>
  WebFont.load({
    google: {
      families: ['Cormorant Garamond:700', 'Lato:300,400,700']
    }
  });
</script>


  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-88810479-1','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>


</body>
</html>
